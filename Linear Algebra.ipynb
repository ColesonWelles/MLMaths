{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5091f649",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca832aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aedafb",
   "metadata": {},
   "source": [
    "## Scalars\n",
    "\n",
    "Most everyday mathematics consists of manipulating numbers one at a time. Formally we call these values **scalars**. We denote scalars by ordinary lower-cased letters (e.g., x, y, and z) and the space of all (continuous) **real-valued scalars** by *R* (*font differnece doesn't come through in markdown*). \n",
    "\n",
    "For now, remember that the expression x ∈ *R* is a formal way to say that x is a real-valued scalar. The symbol ∈ (pronounced \"in\") denotes membership in a set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abf61c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20c77a",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "For now, think of a **vector** as a **fixed-length array** of scalars. As with their code counterparts, we call these scalars the **elements** of the vector (*synonyms include entries and componenets*). Take a real-world example. If we were training a model to predict the risk of a loan defaulting, we might associate each applicant with a vector whose elements correspond to quantities like their income, length of employment, or number of previous defaults.\n",
    "\n",
    "We denote vectors by bold lowercase letters (e.g., **x**, **y**, and **z**). Vectors are implemented as **1st-order tensors**. In general, such tensors can have arbitrary lengths, subject to memory limitations.\n",
    "\n",
    "*A note of caution*. In Python, a vector's indicies start at 0, also known as *zero-based indexing*, whereas in linear algebra subscripts begin at 1 (*one-based indexing*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98aa8e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58946bd7",
   "metadata": {},
   "source": [
    "We can refer to an element of a vector by using a subscript. For example, x<sub>2</sub> denotes the second element of **x**. Since x<sub>2</sub> is a scalar, we don't bold it. By default, we visualize vectors by stacking their elements vertically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2cb2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e33325",
   "metadata": {},
   "source": [
    "In doing so, x<sub>1</sub> . . . x<sub>n</sub> are elements of the vector. Later on, we will distinguish between such **column vectors** and **row vectors** whose elements are stacked horizontally. To indicate that a vector contains *n* elements, we would write **x ∈ R<sup>n</sup>**. Formally we call *n* the **dimensionality** of the vector.\n",
    "\n",
    "In code, this is related to the tensor's length which we can access using Python's `len` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da23617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43b489",
   "metadata": {},
   "source": [
    "We can also access the length via the `shape` attribute. The shape is a tuple that indicates a tensor's length along each axis. Tensors with just one axis have shapes with just one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40aad30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a92f4f",
   "metadata": {},
   "source": [
    "Oftentimes, the word \"dimension\" gets overloaded to mean both the number of axes and the length along a particular axis. To avoid this confusion, it's best to use **order** to refer to the number of axes and **dimensionality** exclusively to refer to the number of components. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ef84b",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "Just as scalars are 0th-order tensors and vectors are 1st-order tensors, matrices are **2nd-order** tensors. We denote matrices by bold capital letters (e.g., **X**, **Y**, and **Z**), and represent them in code using tensors with two axes. \n",
    "\n",
    "The expression **A** ∈ R<sup>m x n</sup> indicates that a matrix **A** contains *m x n* real-valued scalars, arranged as *m rows* and *n columns*. When m = n, we say that a matrix is **square**. Visually, we can illustrate any matrix as a table.\n",
    "\n",
    "To refer to an individual element, we subscript both the row and column indices, e.g., **a**<sub>ij</sub> is the value that belongs to **A**'s i<sup>th</sup> row and j<sup>th</sup> column. In code we can represent a matrix **A** ∈ R<sup>m x n</sup> by a 2nd-order tensor with a shape (m, n). We can convert any appropriately sized *m x n* tensor into an *m x n* matrix by passing the desired shape to `reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbbf0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3, 2)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef84bb",
   "metadata": {},
   "source": [
    "Sometimes we want to flip the axes. When we exchange a matrix's rows and columns, the result is called its **transpose**. Formally, we signify a matrix **A**'s transpose by **A**<sup>T</sup> and if **B** = **A**<sup>T</sup>, then b<sub>ij</sub> = a<sub>ij</sub> for all i and j. Thus, the transpose of an *m x n* matrix is an *n x m* matrix. \n",
    "\n",
    "In code, we can view a matrix's transpose as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6b9f906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba4f0e",
   "metadata": {},
   "source": [
    "Symmetric matrices are the subset of square matrices that are equal to their own transpose: **A** = **A**<sup>T</sup>. Matrices are useful for representing datasets. Typically rows correspond to individual records and columns correspond to distinct attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6701a8",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Tensors give us a generic way of describing extensions to n<sup>th</sup>-order arrays. We call Python objects of the tensor class **tensors** specifically because they too can have arbitrary numbers of axes. Whilte it may be confusing to use the word tensor for both the mathematical object and its implementation in code, the meaning should be clear from the context.\n",
    "\n",
    "We denote general tensors by capital letters with a special, skinnier, font face (e.g., `X`, `Y`, and `Z`) and their indexing mechanism (e.g., *x*<sub>ijk</sub> and `[X]`<sub>1,2i - 1,3</sub>) follows naturally from that of matrices.\n",
    "\n",
    "Tensors will become even more important when we start working with inputs like images. Each image arrives as a 3rd-order tensor with axes corresponding to the height, width, and channel. At each spatial location, the intensities of each color are stacked along the channel. Furthermore, a collection of images is represented in code by a 4th-order tensor, where distinct images are indexed along the first axis. Higher-order tensors are constructed, as were with vectors and matrices, by growing the number of `shape` components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b4e9cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f845420",
   "metadata": {},
   "source": [
    "## Basic Properties of Tensor Arithmetic\n",
    "\n",
    "Scalars, vectors, matrices, and higher-order tensors all share some handy properties. For example, elementwise operations produce outputs that have the same shape as their operands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9857adf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 0.,  2.,  4.],\n",
       "         [ 6.,  8., 10.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone() # Assigns a copy of A to B by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd5874",
   "metadata": {},
   "source": [
    "The elementwise product of two matrices is called their **Hadamard product**, denoted by a *dot enclosed by a circle*. We can spell out the entries of the Hadamard product of two matrices **A**,**B** ∈ R<sup>m x n</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df1be87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234c35c",
   "metadata": {},
   "source": [
    "Adding or multiplying a scalar and a tensor produces a result with the same shape as the original tensor. Here, each element of the tensor is added to (or multiplied by) the scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cda6f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991497e",
   "metadata": {},
   "source": [
    "## Reduction\n",
    "\n",
    "Often, we wish to calculate the sum of a tensor's elements. To express the sum of the elements in a vector **x** of length *n*, we write Σ<sup>n</sup><sub>i=1</sub> x<sub>i</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34da6c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ca6ec",
   "metadata": {},
   "source": [
    "To express sums over the elements of tensors of arbitrary shape, we simply sum over all its axes. For example, the sum of the elements of an *m x n* matrix. **A** could be written Σ<sup>m</sup><sub>i=1</sub> Σ<sup>n</sup><sub>j=1</sub> a<sub>ij</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fff7e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), tensor(15.))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20f4ca",
   "metadata": {},
   "source": [
    "By default, invoking the sum function *reduces* a tensor along all of its axes, eventually producing a scalar. Python libraries also allow us to specify the axes along which the tensor should be reduced.\n",
    "\n",
    "To sum over all elements along the rows (axis0), we specift `axis=0` in `sum`. Since the input matrix reduces along axis 0 to generate the output vector, this axis is missing from the shape of the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "034c01b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3834fc",
   "metadata": {},
   "source": [
    "Specifying `axis=1` will reduce the column dimension (axis 1) by summing up elements of all the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a005e8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396947ca",
   "metadata": {},
   "source": [
    "Reducing a matrix along both rows and columns via summation is equivalent to summing up all the elements of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "947ea96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1]) == A.sum() # Same as A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af5e74",
   "metadata": {},
   "source": [
    "A related quantity is the **mean**, also called the *average*. We calculated the mean by dividing the sum by the total number of elements. Because computing the mean is so common, it gets a dedicated library function that works analogously to sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44883e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.5000), tensor(2.5000))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc2837",
   "metadata": {},
   "source": [
    "Likewise, the function for calculating the mean can also reduce a tensor along specific axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78cac26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2197fb7",
   "metadata": {},
   "source": [
    "## Non-Reduction Sum\n",
    "\n",
    "Sometimes it can be useful to keep the numbers of axes unchanged when invoking the function for calculating the sum or mean. This matters when we want to use the broadcast mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72c55d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.],\n",
       "         [12.]]),\n",
       " torch.Size([2, 1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A, sum_A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a4606",
   "metadata": {},
   "source": [
    "For instance, since `sum_A` keeps its two axes after summing each row, we can divide A by `sum_A` with broadcasting to create a matrix where each row sums up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dab277c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3333, 0.6667],\n",
       "        [0.2500, 0.3333, 0.4167]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3508187",
   "metadata": {},
   "source": [
    "Under certain conditions, even when shapes differ, we can still perform elementwise binary operations by invoking the **broadcasting mechanism**. Broadcasting works according to the following two-step procedure:\n",
    "\n",
    "1) expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape.\n",
    "\n",
    "2) perform an elementwise operation on the resulting arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eede819d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11348223",
   "metadata": {},
   "source": [
    "For example, since `a` and `b` are 3 x 1 and 1 x 2 matrices, respectively, their shapes do not match up. Broadcasting produces a larger 3 x 2 matrix by replicating matrix `a` along the columns and matrix `b` along the rows before adding them elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1e0a8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da994a2",
   "metadata": {},
   "source": [
    "If we want to calculate the cumulative sum of elements of `A` along some axis, say `axis=0` (row by row), we can call the `cumsum` function. By design, this function does not reduce the input tensor along any axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cbc5240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 5., 7.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478b0dc",
   "metadata": {},
   "source": [
    "## Dot Products\n",
    "\n",
    "We've only just performed elementwise operations, sums, and averages so far. Fortunately, this is where things start to get much more interesting. One of the fundamental operations is the **dot product**. Given two vectors **x**,**y** ∈ R<sup>d</sup>, their dot product **x**<sup>T</sup>**y** (also known as *inner product*, <**x**,**y**>) is a sum over the products of the elements at the same position: **x**<sup>T</sup>**y** = Σ<sup>d</sup><sub>i=1</sub> x<sub>i</sub>y<sub>i</sub> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2beed0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(3, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c8356",
   "metadata": {},
   "source": [
    "Equivalently, we can calculate the dot product of two vectors by performing elementwise multiplication followed by a sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "413d17e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f992330",
   "metadata": {},
   "source": [
    "Dot products are useful in a wide range of contexts. For example, given some set of values, denoted by a vector **x** ∈ R<sup>n</sup>, and a set of weights, denoted by **w** ∈ R<sup>n</sup>, the weighted sum of the values in **x** according to the weights **w** could be expressed as the dot product **x**<sup>T</sup>**w**. \n",
    "\n",
    "When the weights are nonnegative and sum to 1, i.e., (Σ<sup>n</sup><sub>i=1</sub> w<sub>i</sub> = 1), the dot productexpresses a **weighted average**. After normalizing two vectors to have unit length, the dot products express the **cosine of the angle between them**. Later we will formally introduce the notion of **length**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0aa5d1",
   "metadata": {},
   "source": [
    "## Matrix-Vector Products\n",
    "\n",
    "Now that we know how to calculate dot products, we can begin to understand the *product* between an *m x n* matrix **A** and an n-dimensional vector **x**. To start off, we visualize our matrix in terms of its row vectors where each **a**<sup>T</sup><sub>i</sub> ∈ R<sup>n</sup> is a row vector representing the i<sup>th</sup> row of the matrix **A**. \n",
    "\n",
    "The matrix-vector product **Ax** is simply a column vector of length *m*, whose i<sup>th</sup> element is the dot product **a**<sup>T</sup><sub>i</sub>**x**:\n",
    "\n",
    "We can also think of multiplication with matrix **A** ∈ R<sup>m x n</sup> as a transformation that projects vectors from R<sup>n</sup> to R<sup>m</sup>. These transformations are remarkably useful. For example, we can represent rotations as multiplications by certain square matrices. Matrix-vector products also describe the key calculation involved in computing the outputs of each layer in a neural network given the outputs from the previous layer. \n",
    "\n",
    "To express a matrix-vector product in code, we use the `mv` function. Note that the column dimension of *A* (its length along axis 1) must be the same as the dimension of x (its length). Python has a **convenience operator `@`** that can execute both matrix-vector and matrix-matrix products (depending on its arguments) Thus we can write *A*@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97b7e9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x), A@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd3e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
