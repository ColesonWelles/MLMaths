{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebbd297a",
   "metadata": {},
   "source": [
    "# Calculus\n",
    "\n",
    "For a long time, how to calculate the area of a circle remained a mystery. Then, in Ancient Greece, the mathematician Archimedes came up with the clever idea to inscribe a series of polygons with increasing numbers of vertices on the inside of a circle.\n",
    "\n",
    "For a polygon with *n* vertices, we obtain *n* triangles. The height of each triangle approaches the radius *r* as we partition the circle more finely. At the same time, its base approaches 2ùúãùëü/ùëõ, since the ratio between arc and secant approaches 1 for a large number of vertices. Thus, the area of the polygon approaches *n* * *r* * 1/2(2ùúãùëü/ùëõ) = ùúãùëü<sup>2</sup>. This can also be described as finding the area of a circle by a limit procedure. \n",
    "\n",
    "This limit procedure is at the root of both **differential calculus** and **integral calculus**. The former can tell us how to increase or decrease a function's value by manipulating its arguments. This comes in handy for the *optimization problems* that we face in deep learning, where we repeatedly update our paramters in order to decrease the loss function. Optimization addresses how to fit our models to training data, and calculus is its key prerequisite. However, it's important not to forget that our ultimate goal is to perform well on **previously unseen** data. That problem is called **generalization**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193ff056",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline import backend_inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba66f1e",
   "metadata": {},
   "source": [
    "## Derivatives and Differentiation\n",
    "\n",
    "Simply put, a **derivative** is a rate of change in a function with respect to changes in its arguments. Derivatives can tell us how rapidly a loss function would increase or decrease were we to increase or decrease each parameter by an infintesimally small amount. Formally, for functions f: R --> R, that map from scalars to scalars, the derivative of f at a point x is defined as f<sup>'</sup>(x) = lim<sub>h->0</sub> f(x + h) - f(x) / h\n",
    "\n",
    "This term on the right hand side is called a **limit** and it tells us what happens to the value of an expression as a specified variable approaches a particular value. This limit tells us what the ratio between a pertubation *h* and the change in the function value f(x + h) - f(x) converges to as we shrink its size to zero. \n",
    "\n",
    "When f<sup>'</sup>(x) exists, f is said to be **differentiable** at x: and when *f*<sup>'</sup>(x) exists for all x on a set, e.g., the interval [a,b], *we say that *f is differentiable on this set.* Not all functions are differentiable, including many that we wish to optimize, such as accuracy and the area under the receiving operating characteristic (AUC). However, because computing the derivative of the loss is a crucial step in nearly all algorithms for training deep neural networks, we often optimize a **differentiable surrogate** instead. \n",
    "\n",
    "We can interpret the derivative f<sup>'</sup>(x) as the **instantaneous rate of change** of f(x) with respect to x. Let's solidify our understanding with an example. Let's define u = f(x) = 3x<sup>2</sup> - 4x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf577f42-f990-4b9b-95bb-cc4c03b5ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21c77a",
   "metadata": {},
   "source": [
    "Setting x = 1, we see that f(x + h) - f(x) / h approaches 2 as h approaches 0. While this experiment lacks the rigor of a mathematical proof, we can quickly see that indeed f<sup>'</sup>(1) = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c45c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit=2.30000\n",
      "h=0.01000, numerical limit=2.03000\n",
      "h=0.00100, numerical limit=2.00300\n",
      "h=0.00010, numerical limit=2.00030\n",
      "h=0.00001, numerical limit=2.00003\n"
     ]
    }
   ],
   "source": [
    "for h in 10.0**np.arange(-1, -6, -1):\n",
    "    print(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80270157",
   "metadata": {},
   "source": [
    "There are several equivalent notational conventions for derivatives. Given y = f(x), the following expressions are equivalent: f<sup>'</sup>(x) = y<sup>'</sup> = (dy / dx) = (df / dx) = (d / dx) f(x) = Df(x) = D<sub>x</sub>f(x),\n",
    "\n",
    "where the symbols d / dx and D are **differentiation operators**. Below, we present the derivatives of some common functions:\n",
    "\n",
    "(d / dx)C = 0\n",
    "\n",
    "(d / dx)x<sup>n</sup> = nx<sup>n - 1</sup> for n ‚â† 0\n",
    "\n",
    "(d / dx)e<sup>x</sup> = e<sup>x</sup>\n",
    "\n",
    "(d / dx)ln x = x<sup>-1</sup>\n",
    "\n",
    "Functions composed from differentiable functions are often themselves differentiable. The following rules come in handy for working with compositions of any differetial functions f and g, and constant C. See the **Constant multiple rule**, the **Sum rule**, the **Product rule**, and the **Quotient rule**. \n",
    "\n",
    "Using this, we can apply the rules to find the derivative of 3x<sup>2</sup> - 4x via: (d / dx)[3x<sup>2</sup> - 4x] = 3(d / dx)x<sup>2</sup> - 4(d / dx)x = 6x - 4.\n",
    "\n",
    "Plugging in x = 1 shows that, indeed, the derivative equals 2 at this location. Not that derivatives tell us the **slope** of a function at a particular location. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b7097",
   "metadata": {},
   "source": [
    "## Visualization Utilities\n",
    "\n",
    "We can visualize the slopes of functions using the `matplotlib` library. We need to define a few functions. As its name indicates, `use_svg_display` tells `matplotlib` to output graphics in SVG format for crisper images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f513eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_svg_display():\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a7cca",
   "metadata": {},
   "source": [
    "Conveniently, we can set figure sizes with `plt.rcParams['figure.figsize'] = figsize` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7ea41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale), axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim), axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006f308",
   "metadata": {},
   "source": [
    "The `set_axes` function can associate axes with properties, including labels, ranges, and scales.\n",
    "\n",
    "With these three functions, we can definite a `plot` function to overlay multiple curves. Much of the code here is just ensuring that the sizes and shapes of the inputs match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8621b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None, \n",
    "        ylim=None, xscale='linear', yscale='linear', \n",
    "        fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"Plot data points\"\"\"\n",
    "    \n",
    "    if isinstance(X, np.ndarray) and X.ndim == 1:\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif isinstance(Y, np.ndarray) and Y.ndim == 1:\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    if axes is None:\n",
    "        axes = plt.gca()\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        axes.plot(x, y, fmt) if len(x) else axes.plot(y, fmt)\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    if xlim:\n",
    "        axes.set_xlim(xlim)\n",
    "    if ylim:\n",
    "        axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f315be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, 3, 0.1)\n",
    "y = f(x)\n",
    "tangent_line = 2 * x - 3\n",
    "\n",
    "plot(x, [y, tangent_line], xlabel='x', ylabel='f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93120ea9",
   "metadata": {},
   "source": [
    "## Partial Derivatives and Gradients\n",
    "\n",
    "So far, we've been differentiating functions of just one variable. In deep learning, we need to be able to work with functions of *many* variables. Let's briefly touch on the basics of the derivative related to **multivariate** functions. \n",
    "\n",
    "Let y = f(x<sub>1</sub>,x<sub>2</sub>, . . .,x<sub>n</sub>) be a function with *n* variables. The **partial derivative** of y with respect to its i<sup>th</sup> parameter x<sub>i</sub> is:\n",
    "\n",
    "(ùúïùë¶ / ùúïx<sub>i</sub>) = lim<sub>h-->0</sub> f(x<sub>1</sub>, . . .,x<sub>i - 1</sub>, x<sub>i</sub> + h, x<sub>i + 1</sub>, . . .,x<sub>n</sub>) - f(x<sub>1</sub>, . . .,x<sub>i</sub>, . . .,x<sub>n</sub>) / h\n",
    "\n",
    "To calculate (ùúïùë¶ / ùúïx<sub>i</sub>), we can treat x<sub>1</sub>, . . .,x<sub>i - 1</sub>, x<sub>i + 1</sub>, . . .,x<sub>n</sub> as constants and calculate the derivative of y with respect to x<sub>i</sub>. The following notational conventions for partial derivatives are all common and all mean the same thing: \n",
    "\n",
    "(ùúïùë¶ / ùúïx<sub>i</sub>) = (ùúïf / ùúïx<sub>i</sub>) = ùúï<sub>x<sub>i</sub></sub>f = ùúï<sub>i</sub>f = f<sub>x<sub>i</sub></sub> = f<sub>i</sub> = D<sub>i</sub>f = D<sub>x<sub>i</sub></sub>f.\n",
    "\n",
    "We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain a vector that is called the **gradient** of the function. Suppose that the input of function f : R<sup>n</sup> --> R is an n-dimensional vector **x** = [x<sub>1</sub>,x<sub>2</sub>, . . .,x<sub>n</sub>]<sup>T</sup> and the output is a scalar. The gradient of the function f with respect to x is a vector of n partial derivatives:\n",
    "\n",
    "‚àá<sub>x</sub>f(**x**) = [ùúï<sub>x<sub>1</sub></sub>f(**x**),ùúï<sub>x<sub>2</sub></sub>f(**x**), . . . ùúï<sub>x<sub>n</sub></sub>f(**x**)]<sup>T</sup>\n",
    "\n",
    "When there is no ambiguity, ‚àá<sub>x</sub>f(**x**) is typically replaced by ‚àáf(**x**). The following rules come in handy for differentiating multivariate functions:\n",
    "\n",
    "1) For all **A** ‚àà R<sup>m x n</sup> we have  ‚àá<sub>x</sub>**Ax** = **A**<sup>T</sup> and ‚àá<sub>x</sub>X<sup>T</sup>**A** = **A**.\n",
    "\n",
    "2) For square matrices **A** ‚àà R<sup>m x n</sup> we have that ‚àá<sub>x</sub>X<sup>T</sup>**Ax** = (**A** + **A**<sup>T</sup>)x and in particular ‚àá<sub>x</sub>||X||<sup>2</sup> = ‚àá<sub>x</sub>X<sup>T</sup>**x** = 2**x**.\n",
    "\n",
    "Similarly, for any matrix **X**, we have ‚àá<sub>x</sub>||**X**||<sup>2</sup><sub>F</sub> = 2**X**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba8252",
   "metadata": {},
   "source": [
    "## The Chain Rule\n",
    "\n",
    "In deep learning, the gradients of concern are often difficult to calculate because we are working with deeply nested functions(of functions(of functions...)). Fortunately for us, **the chain rule** of calculus takes care of this. \n",
    "\n",
    "Recalling functions of a single variable, suppose that y = f(g(x)) and the underlying function y = f(u) and u = g(x) are both differentiable. The chain rule states that: \n",
    "\n",
    "(dy / dx) = (dy / du) * (du / dx)\n",
    "\n",
    "Returning to the notion of multivariate functions, suppose that y = f(**u**) has variables u<sub>1</sub>, u<sub>2</sub>, . . .,u<sub>m</sub>, where each u<sub>i</sub> = g<sub>i</sub>(**x**) has variables x<sub>1</sub>, x<sub>2</sub>, . . .,x<sub>n</sub>, i.e., **u** = g(**x**). Then the chain rule states that:\n",
    "\n",
    "(ùúïùë¶ / ùúïx<sub>i</sub>) = (ùúïùë¶ / ùúïu<sub>1</sub>) * (ùúïu<sub>1</sub> / ùúïx<sub>i</sub>) + (ùúïùë¶ / ùúïu<sub>2</sub>) * (ùúïu<sub>2</sub> / ùúïx<sub>i</sub>) + . . . + (ùúïùë¶ / ùúïu<sub>m</sub>) * (ùúïu<sub>m</sub> / ùúïx<sub>i</sub>)\n",
    "\n",
    "and so ‚àá<sub>x</sub>y = **A**‚àá<sub>u</sub>y, where **A** ‚àà R<sup>n x m</sup> is a matrix that contains the derivative of vector **u** with respect to the vector **x**. Thus, evaluating the gradient requires computing a vector-matrix product. This is one of the key reasons why linear algebra is such an integral building block for deep learning systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f467d",
   "metadata": {},
   "source": [
    "## Calculus Recap\n",
    "\n",
    "We've only scratched the surface of the incredibly deep topic that is calculus, however the relevance of some concepts should be starting to click. The composition of rules for differentiation can be applied routinely, enabling us to compute gradients *automatically* without any particular creativity on our part. Moreover, computing the derivatives of vector-valued functions requires us to multiply matrices as we trace the dependency graph of variables from output to input. \n",
    "\n",
    "This graph is traversed in a **forward** direction when we evaluate the function and in a **backwards** direction when we compute gradients. We're starting to rub up against the concept of **backpropagation**, a computational way of applying the chain rule of calculus. \n",
    "\n",
    "With regards to optimization, gradients allow us to determine how to move the parameters of a model in order to lower the loss, and each step of the optimization algorithms used throughout this book will require calculating the gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14276d99",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "Despite the relative straightforwardness of calculating derivatives, doing so by hand is tedious and subject to human-induced errors and it only get's worse the more complex a given model becomes. Fortunately all modern deep learning frameworks have what's called **automatic differentiation**, also known as **autograd**. \n",
    "\n",
    "As we pass data through each successive function, the framework builds a **computational graph** that tracks how each value depends on others. To calculate derivatives, autograd works backwards through this graph by applying the chain rule. The computational algorithm for applying the chain rule like this is the aforementioned **backpropaagation**. \n",
    "\n",
    "While most autograd implementations themselves are quite modern, they do have a somewhat long history to them. The earliest references to autograd date back to Wengert (1964). The core ideas behind more modern backpropagation date to a PhD thesis from 1980 (Speelpenning, 1980) and were further developed in the late 1980s (Griewank, 1989). Backpropagation has become the default method for computing gradients, but it's technically not the only option. For example, the programming language Julia employs *forward propagation* (Revels et al.,  2016). \n",
    "\n",
    "Let's try to cement our understanding of autograd as implemented in PyTorch's library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f281f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727ba4c",
   "metadata": {},
   "source": [
    "Let's assume that we're interested in differentiating the function y = 2**x**<sup>T</sup>**x** with respect to the column vector **x**. To begin, we assign x an initial value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b27319a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c97f6",
   "metadata": {},
   "source": [
    "Before we start calculating the gradient of y with respect to **x**, we need a place to store it. Generally speaking, we avoid allocating new memory every time we take a derivative because deep learning requires successively computing derivatives with respect to the same parameters many, many times and that could get out of hand quickly from a memory management point of view. \n",
    "\n",
    "Note that the gradient of a scalar-valued function with respect to vector x is a vector-valued with the same shape as **x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b4bb1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also be written as x = torch.arange(4.0, requires_grad=True)\n",
    "x.requires_grad_(True)\n",
    "x.grad # The gradient is None by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68678bc4",
   "metadata": {},
   "source": [
    "We now calculate our function of x and assign the results to y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2df3a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340fcf4",
   "metadata": {},
   "source": [
    "We can now take the gradient of y with respect to x by calling its `backward` method. Next, we can access the gradient via x's `grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01bd5310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e2fc6",
   "metadata": {},
   "source": [
    "We already know that the gradient of the function y = 2x<sup>T</sup>x with respect to x should be 4x. We can now verify that the automatic gradient computation and the expected result are identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae191e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219419dc",
   "metadata": {},
   "source": [
    "Now let's calculate another function of x and take its gradient. Keep in mind that PyTorch does not automatically reset the gradient buffer when we record a new gradient. Instead, the new gradient is added to the already-stored gradient. This functionality comes in handy when we want to optimize the sum of multiple objective functions. In order to reset the gradient buffer, we can call `x.grad.zero_()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9243ac8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # Resets the gradient buffer\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fc768",
   "metadata": {},
   "source": [
    "## Backward for Non-scalar Variables\n",
    "\n",
    "When y is a vector, the most natural representation of the derivative of y with respect to a vector x is a matrix called **the Jacobian** that contains the partial derivatives of each component of y with respect to each component of x. Likewise, for higher-order y and x, the result of differentiation could be an even higher-order tensor. \n",
    "\n",
    "While Jacobians do show up in some advanced machine learning methods, more commonly we'd rather sum up the gradients of each component of y with respect to the full vector x, giving us a vecotr of the same shape as x. For example, we often have a vector representing the value of our loss function calculated seperately for each example among a **batch** of training examples. Here, we just want to sum up the gradients that were computed individually for each example. \n",
    "\n",
    "Because deep learning frameworks vary in how they interpret gradients of non-scalar tensors, PyTorch takes some steps to avoid confusion. Invoking `backward` on a non-scalar creates an error unless we tell PyTorch how to reduce the object to a scalar. Formally put, we need to provide some vector **v** such that `backward` will compute **v**<sup>T</sup>ùúï<sub>x</sub>**y** rather than ùúï<sub>x</sub>**y**. \n",
    "\n",
    "This might be starting to get a little confusing, but we'll attempt to clarify later on. This argument (representing **v**) is named `gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76323ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.backward(gradient=torch.ones(len(y))) # Faster: y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d664de",
   "metadata": {},
   "source": [
    "## Detaching Computation\n",
    "\n",
    "Sometimes we wish to move some calculation outside of the recorded computational graph. For instance, suppose we use the input to create some auxiliary intermediate terms for which we do not want to compute a gradient. In this scenario, we need to **detach** the respective computational graph from the final result. \n",
    "\n",
    "The following toy example makes this more comprehendable. Say we have z = x * y and y = x * x but we want to focus on the *direct* influence of x on z rather than the influence conveyed via y. We can then create a new variable, u, that takes the same value as y but whose **provenance** (how it was created) has been wipted out. Thus u has no ancestors in the graph and gradients do not flow through u to x. \n",
    "\n",
    "For instance, taking the gradient of z = x * u will yield the result u, (not 3 * x * x as you would otherwise expect since z = x * x * x). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35fb5aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4059a",
   "metadata": {},
   "source": [
    "While this procedure detaches y's ancestors from the graph that leading to z, the computational graph leading to y persists and thus we can calculate the gradient of y with respect to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d83922b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae9b29",
   "metadata": {},
   "source": [
    "## Gradients and Python Control Flow\n",
    "\n",
    "So far we reviewed cases where the path from input to output was well defined via a function such as z = x * x * x. Programming offers us a lot more freedom in how we compute results. For example, we can make them depend on auxiliary variables or condition choices on intermediate results. One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable. \n",
    "\n",
    "To illustrate this, consider the following code snippet where the number of iterations of the `while` loop and the evaluation of the `if` statement both depend on the value of the input `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc29f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3582a",
   "metadata": {},
   "source": [
    "In the next code snippet, we call this function and pass in a random value as input. Since th input is a random variable, we do not know what form the computational graph will take. However, whenever we execute `f(a)` on a specific input, we realize a specific computational graph and can subsequently run `backward`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bbf7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432428f",
   "metadata": {},
   "source": [
    "Even though our function `f` is, in this example, a bit contrived it nevertheless has a dependence on the input is quite simple: it is a *linear* function of `a` with piecewise defined scale. As such, `f(a) / a` is a vector of constant entries and, moreover, `f(a) / a` needs to match the gradient of `f(a)` with respect to `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed67aec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbed4d",
   "metadata": {},
   "source": [
    "Dynamic control flow is very common in deep learning. For example, when processing text, the computational graph depends on the length of the input. In these cases, automatic differentiation becomes vital for statistical modeling since **it is impossible to compute the gradient a priori**. \n",
    "\n",
    "## Automatic Differentiation Recap \n",
    "\n",
    "The power of automatic differentiation should now be coming into focus. The developement of libraries for calculating derivatives both automatically and efficiently has been a massive productivity booster for deep learning practictioners. \n",
    "\n",
    "Autograd allows us to design models for which pen and paper computations would be prohibitively time consuming and unacceptably prone to human-induced error. While we use autograd libraries to optimize our models (statistically speaking), the optimization of autograd libraries themselves (computationally speaking) is a rich subject that continues to merit further work and research. \n",
    "\n",
    "The basic steps to keep in mind going forward are as follows:\n",
    "\n",
    "1) Attach gradients to those variables with respect to which we desire derivatives; \n",
    "\n",
    "2) record the computation of the target value;\n",
    "\n",
    "3) execute the backpropagation function \n",
    "\n",
    "4) access the resulting gradient "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
